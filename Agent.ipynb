{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environments.ipynb\n",
      "importing Jupyter notebook from Actor_Critic.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "import torch  \n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from scipy.stats import beta\n",
    "\n",
    "import statistics\n",
    "\n",
    "import import_ipynb\n",
    "from Environments import Square_Crossroads\n",
    "from Actor_Critic import Actor_Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reset with param\n",
      "Environment reset with param\n",
      "{0: [[0, 5], [5, 10], 1, 1], 1: [[5, 0], [0, 5], 1, 1], 2: [[5, 10], [5, 0], 1, 1], 3: [[10, 5], [5, 0], 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "n_times_steps = 3\n",
    "seed = 10\n",
    "dist_cars = 0.1\n",
    "\n",
    "env = Square_Crossroads(seed, dist_cars)\n",
    "env.reset()\n",
    "print(env.states)\n",
    "# action_space = {0:[0,0], 1:[0,0], 2:[0,0], 3:[0,0]}\n",
    "# total_reward = 0\n",
    "\n",
    "# state_tensor = torch.zeros([4,2], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ac = Actor_Critic(state_tensor, 20, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(advantage, delta = 1.0):\n",
    "    \n",
    "    '''\n",
    "    Huber loss, given target and prediction\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    error = abs(advantage)\n",
    "    \n",
    "    \n",
    "    if error.item() > delta:\n",
    "        loss = 0.5 * delta ** 2 + delta * (error - delta)\n",
    "\n",
    "    else: \n",
    "        loss = 0.5 * error ** 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"hidden_size_actor\" : 200,\n",
    "                   \"hidden_size_critic\" : 400,\n",
    "                   \"lr\" : 5e-4,\n",
    "                   \"step_size\" : 5,\n",
    "                   \"gamma\" : 0.9}\n",
    "\n",
    "max_episodes = 200\n",
    "num_steps = 5000\n",
    "GAMMA = 0.99\n",
    "delta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, hyperparameters):\n",
    "    \n",
    "    model = Actor_Critic(2, hyperparameters[\"hidden_size_actor\"], hyperparameters[\"hidden_size_critic\"])\n",
    "    model_optimizer = optim.Adam(model.parameters(), lr = hyperparameters[\"lr\"])\n",
    "    model_scheduler = optim.lr_scheduler.StepLR(model_optimizer, step_size = hyperparameters[\"step_size\"], gamma = hyperparameters[\"gamma\"])\n",
    "    \n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    l_actor = []\n",
    "    l_critic = []\n",
    "    l_total = []\n",
    "    all_reward = []\n",
    "    for episode in range(max_episodes):\n",
    "        print(\"Training episode: \", episode)\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        env.reset()\n",
    "        \n",
    "        states = [v[0] for v in env.states.values()] \n",
    "        state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                    [states[1][0], states[1][1]],\n",
    "                                    [states[2][0], states[2][1]],\n",
    "                                    [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "\n",
    "        for steps in tqdm(range(num_steps)):\n",
    "            \n",
    "            log_prob = 0\n",
    "            actions = []\n",
    "            \n",
    "            value, policy_dist_alpha, policy_dist_beta, sign = model.forward(state_space)\n",
    "            \n",
    "#             print(policy_dist_alpha.requires_grad)\n",
    "#             print(value.requires_grad)\n",
    "#             print(policy_dist_beta.requires_grad)\n",
    "#             print(sign.requires_grad)\n",
    "            \n",
    "            \n",
    "                \n",
    "            value = value.detach().numpy().mean()\n",
    "#             print(value)\n",
    "            for n in range(4):\n",
    "                tmp_alpha = policy_dist_alpha[n].clone()\n",
    "                tmp_beta = policy_dist_beta[n].clone()\n",
    "                \n",
    "                dist = beta(tmp_alpha.detach().numpy(),tmp_beta.detach().numpy())\n",
    "                \n",
    "                action = np.random.beta(tmp_alpha.detach().numpy(), tmp_beta.detach().numpy(), size=2)\n",
    "#                 print(\"These are the actions sampled from the beta distribution: \", action)\n",
    "                log_prob += torch.mean(torch.tensor(dist.logpdf(action)))\n",
    "                \n",
    "                # sign: [P(accelerate), P(decelearte)]\n",
    "#                 print(\"The softmax scores for accelerate-decelarate: \", sign[n])\n",
    "                if torch.argmax(sign[n]) == 1:\n",
    "                    action[0] = -action[0]\n",
    "#                     print(\"Decelerate\")\n",
    "#                 else:\n",
    "#                     print(\"Accelerate\")\n",
    "#                 print(\"This is the accelerate/decelarate action: \", action[0])\n",
    "                actions.append(action)\n",
    "#             \n",
    "#             action_1 = np.random.beta(policy_dist_alpha[0].detach().numpy(), policy_dist_beta[0].detach().numpy(), size=2)\n",
    "#             action_2 = np.random.beta(policy_dist_alpha[1].detach().numpy(), policy_dist_beta[1].detach().numpy(), size=2)\n",
    "#             action_3 = np.random.beta(policy_dist_alpha[2].detach().numpy(), policy_dist_beta[2].detach().numpy(), size=2)\n",
    "#             action_4 = np.random.beta(policy_dist_alpha[3].detach().numpy(), policy_dist_beta[3].detach().numpy(), size=2)\n",
    "            \n",
    "#             print(\"The sign of the acceleration is: \",sign)\n",
    "            \n",
    "            action_space = torch.tensor(actions, dtype=torch.float32)                \n",
    "            \n",
    "            new_state_space, reward, done = env.step(action_space, sign)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob.mean())\n",
    "            \n",
    "            states = [v[0] for v in env.states.values()] \n",
    "\n",
    "            new_state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                            [states[1][0], states[1][1]],\n",
    "                                            [states[2][0], states[2][1]],\n",
    "                                            [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "            state_space = new_state_space\n",
    "\n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _, _, _ = model.forward(state_space)\n",
    "#                 print(\"Qval: \",Qval)\n",
    "                Qval = torch.mean(Qval).detach().numpy()\n",
    "#                 print(\"Qval averaged: \",Qval)\n",
    "                \n",
    "                Qval = Variable(torch.FloatTensor(Qval), requires_grad = True)\n",
    "#                 print(Qval.requires_grad)\n",
    "                \n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "#                 if episode % 10 == 0:                    \n",
    "                sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                print(\"Episode finished\")\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        Qvals = np.zeros_like(values)\n",
    "#         print(\"The rewards are: \",rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "#             print(\"I get these rewards: \",rewards)\n",
    "#             print(\"I use this Qval: \", Qval)\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "#             print(\"Here is a single Qval: \",Qval)\n",
    "            Qvals[t] = Qval\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "#         values = [torch.from_numpy(item).float() for item in values]\n",
    "#     state_tensor = Variable(state_tensor.float())\n",
    "        values = Variable(torch.FloatTensor(values), requires_grad=True)\n",
    "#         print(\"The values are: \",values)\n",
    "        Qvals = Variable(torch.FloatTensor(Qvals), requires_grad=True)\n",
    "#         print(\"The q values are : \", Qvals)\n",
    "        \n",
    "#         print(\"the non stacked log probs are: \", log_probs)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "#         print(\"The log_probs are: \",log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "#         print(\"The advantage is: \",advantage)\n",
    "        \n",
    "        avg_advantage = advantage.mean()\n",
    "#         print(\"The negative logs are: \", -log_probs)\n",
    "#         print(\"The average advantage is: \",avg_advantage)\n",
    "        avg_log_probs = log_probs.mean()\n",
    "#         print(\"The average logs are: \",avg_log_probs)\n",
    "        \n",
    "        loss_actor = (avg_log_probs * advantage).mean()\n",
    "        \n",
    "        loss_critic = huber_loss(avg_advantage, delta)\n",
    "        \n",
    "#         loss_critic = abs(0.5*advantage.mean())\n",
    "        \n",
    "#         loss_actor = (-(policy_dist_alpha.sum() + policy_dist_beta.sum() + 1e-5)) * advantage   \n",
    "        \n",
    "        print(\"cricitc loss is: .........\",loss_critic)\n",
    "        print(\"actor loss is: .........\", loss_actor)\n",
    "        total_loss = loss_actor + loss_critic\n",
    "        \n",
    "        print(\"The total loss is: \",total_loss)\n",
    "        \n",
    "        l_actor.append(loss_actor)\n",
    "        l_critic.append(loss_critic)\n",
    "        l_total.append(total_loss)\n",
    "        \n",
    "        model_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        model_optimizer.step()\n",
    "#         model_scheduler.step()\n",
    "        \n",
    "        all_reward.append(statistics.mean(rewards))\n",
    "\n",
    "    plt.plot(l_actor)\n",
    "    plt.legend(\"Actor loss\")    \n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(l_critic)\n",
    "    plt.legend(\"Critic loss\")    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(l_total)\n",
    "    plt.legend(\"Total loss\")    \n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(all_reward)\n",
    "    plt.legend(\"Mean reward\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(env, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
