{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "import torch  \n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from scipy.stats import beta\n",
    "\n",
    "import statistics\n",
    "import json\n",
    "\n",
    "import import_ipynb\n",
    "from Environments import Square_Crossroads\n",
    "# from Actor_Critic import Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size_actor):\n",
    "        super(Actor, self).__init__()\n",
    "      \n",
    "        self.num_inputs = num_inputs\n",
    "        self.actor_linear1 = nn.Linear(self.num_inputs, hidden_size_actor)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size_actor, hidden_size_actor)\n",
    "        self.alpha = nn.Linear(hidden_size_actor, 1)\n",
    "        self.beta = nn.Linear(hidden_size_actor, 1)\n",
    "        \n",
    "        self.pos_neg_1 = nn.Linear(self.num_inputs, 10)\n",
    "        self.pos_neg_2 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, state_tensor):\n",
    "\n",
    "        policy_dist = F.relu(self.actor_linear1(state_tensor))\n",
    "        policy_dist = F.relu(self.actor_linear2(policy_dist))\n",
    "\n",
    "        policy_dist_alpha = torch.clamp(self.alpha(policy_dist), 0.1, 3)\n",
    "        policy_dist_beta = torch.clamp(self.beta(policy_dist), 0.1, 3)\n",
    "        \n",
    "        sign = F.relu(self.pos_neg_1(state_tensor))\n",
    "        sign = F.softmax(self.pos_neg_2(sign))\n",
    "        \n",
    "        return policy_dist_alpha, policy_dist_beta, sign\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size_critic):\n",
    "        super(Critic, self).__init__()\n",
    "      \n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        \n",
    "        self.critic_linear1 = nn.Linear(self.num_inputs, hidden_size_critic)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size_critic, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state_tensor):\n",
    "        state_tensor = Variable(state_tensor.float())\n",
    "        value = F.relu(self.critic_linear1(state_tensor))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reset with param\n",
      "Environment reset with param\n",
      "{0: [[0, 5], [5, 10], 1, 1], 1: [[5, 0], [0, 5], 1, 1], 2: [[5, 10], [5, 0], 1, 1], 3: [[10, 5], [5, 0], 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "n_times_steps = 3\n",
    "seed = 10\n",
    "dist_cars = 0.1\n",
    "\n",
    "env = Square_Crossroads(seed, dist_cars)\n",
    "env.reset()\n",
    "print(env.states)\n",
    "# action_space = {0:[0,0], 1:[0,0], 2:[0,0], 3:[0,0]}\n",
    "# total_reward = 0\n",
    "\n",
    "# state_tensor = torch.zeros([4,2], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ac = Actor_Critic(state_tensor, 20, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[0, 5], [5, 10], 1, 1],\n",
       " 1: [[5, 0], [0, 5], 1, 1],\n",
       " 2: [[5, 10], [5, 0], 1, 1],\n",
       " 3: [[10, 5], [5, 0], 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Q_actor_critic(env, hyperparameters):\n",
    "    \n",
    "    model = Actor_Critic(2, hyperparameters[\"hidden_size_actor\"], hyperparameters[\"hidden_size_critic\"])\n",
    "    model_optimizer = optim.Adam(model.parameters(), lr = hyperparameters[\"lr\"])\n",
    "    model_scheduler = optim.lr_scheduler.StepLR(model_optimizer, step_size = hyperparameters[\"step_size\"], gamma = hyperparameters[\"gamma\"])\n",
    "    \n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    l_actor = []\n",
    "    l_critic = []\n",
    "    l_total = []\n",
    "    all_reward = []\n",
    "    for episode in range(max_episodes):\n",
    "        print(\"Training episode: \", episode)\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        env.reset()\n",
    "        \n",
    "        states = [v[0] for v in env.states.values()] \n",
    "        state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                    [states[1][0], states[1][1]],\n",
    "                                    [states[2][0], states[2][1]],\n",
    "                                    [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "\n",
    "        for steps in tqdm(range(num_steps)):\n",
    "            \n",
    "            log_prob = 0\n",
    "            actions = []\n",
    "            \n",
    "            value, policy_dist_alpha, policy_dist_beta, sign = model.forward(state_space)\n",
    "            \n",
    "            \n",
    "                \n",
    "            value = value.detach().numpy().mean()\n",
    "#             print(value)\n",
    "            for n in range(4):\n",
    "                tmp_alpha = policy_dist_alpha[n].clone()\n",
    "                tmp_beta = policy_dist_beta[n].clone()\n",
    "                \n",
    "                dist = beta(tmp_alpha.detach().numpy(),tmp_beta.detach().numpy())\n",
    "                \n",
    "                action = np.random.beta(tmp_alpha.detach().numpy(), tmp_beta.detach().numpy(), size=2)\n",
    "                log_prob += torch.mean(torch.tensor(dist.logpdf(action)))\n",
    "                \n",
    "\n",
    "                if torch.argmax(sign[n]) == 1:\n",
    "                    action[0] = -action[0]\n",
    "\n",
    "                actions.append(action)\n",
    "   \n",
    "            action_space = torch.tensor(actions, dtype=torch.float32)                \n",
    "            \n",
    "            new_state_space, reward, done = env.step(action_space, sign)\n",
    "            \n",
    "            \n",
    "            advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob.mean())\n",
    "            \n",
    "            states = [v[0] for v in env.states.values()] \n",
    "\n",
    "            new_state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                            [states[1][0], states[1][1]],\n",
    "                                            [states[2][0], states[2][1]],\n",
    "                                            [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "            state_space = new_state_space\n",
    "\n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _, _, _ = model.forward(state_space)\n",
    "#                 print(\"Qval: \",Qval)\n",
    "                Qval = torch.mean(Qval).detach().numpy()\n",
    "#                 print(\"Qval averaged: \",Qval)\n",
    "                \n",
    "                Qval = Variable(torch.FloatTensor(Qval), requires_grad = True)\n",
    "#                 print(Qval.requires_grad)\n",
    "                \n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "#                 if episode % 10 == 0:                    \n",
    "                sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                print(\"Episode finished\")\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        Qvals = np.zeros_like(values)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "            Qvals[t] = Qval\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "\n",
    "        # suppose everything have the correct type\n",
    "        # the term 'done' is important because for the end of the episode we only want\n",
    "        # the reward, without the discounted next state value.\n",
    "        advantage = reward + (1.0 - done) * gamma * critic(next_state) - critic(state)\n",
    "\n",
    "        values = Variable(torch.FloatTensor(values), requires_grad=True)\n",
    "        Qvals = Variable(torch.FloatTensor(Qvals), requires_grad=True)\n",
    "        \n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "        \n",
    "        avg_advantage = advantage.mean()\n",
    "        avg_log_probs = log_probs.mean()\n",
    "        \n",
    "        loss_actor = (avg_log_probs * advantage).mean()\n",
    "        \n",
    "        loss_critic = huber_loss(avg_advantage, delta)\n",
    "        \n",
    "#         loss_critic = abs(0.5*advantage.mean())\n",
    "        \n",
    "#         loss_actor = (-(policy_dist_alpha.sum() + policy_dist_beta.sum() + 1e-5)) * advantage   \n",
    "        \n",
    "        print(\"cricitc loss is: .........\",loss_critic)\n",
    "        print(\"actor loss is: .........\", loss_actor)\n",
    "        total_loss = loss_actor + loss_critic\n",
    "        \n",
    "        print(\"The total loss is: \",total_loss)\n",
    "        \n",
    "        l_actor.append(loss_actor)\n",
    "        l_critic.append(loss_critic)\n",
    "        l_total.append(total_loss)\n",
    "        \n",
    "        model_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        model_optimizer.step()\n",
    "#         model_scheduler.step()\n",
    "        \n",
    "        all_reward.append(statistics.mean(rewards))\n",
    "\n",
    "    plt.plot(l_actor)\n",
    "    plt.legend(\"Actor loss\")    \n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(l_critic)\n",
    "    plt.legend(\"Critic loss\")    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(l_total)\n",
    "    plt.legend(\"Total loss\")    \n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(all_reward)\n",
    "    plt.legend(\"Mean reward\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"hidden_size_actor\" : 200,\n",
    "                   \"hidden_size_critic\" : 400,\n",
    "                   \"lr_actor\" : 1e-4,\n",
    "                   \"lr_critic\" : 1e-4,\n",
    "                   \"step_size\" : 5,\n",
    "                   \"gamma\" : 0.9}\n",
    "\n",
    "max_episodes = 3\n",
    "GAMMA = 0.99\n",
    "delta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(advantage, delta = 1.0):\n",
    "    \n",
    "    '''\n",
    "    Huber loss, given advantage\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    error = abs(advantage).sum()\n",
    "    \n",
    "    if error.item() > delta:\n",
    "        loss = 0.5 * delta ** 2 + delta * (error - delta)\n",
    "\n",
    "    else: \n",
    "        loss = 0.5 * error ** 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_A2C(env, hyperparameters):\n",
    "    \n",
    "    actor = Actor(2, hyperparameters[\"hidden_size_actor\"])\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr = hyperparameters[\"lr_actor\"])\n",
    "    actor_scheduler = optim.lr_scheduler.StepLR(actor_optimizer, step_size = hyperparameters[\"step_size\"], gamma = hyperparameters[\"gamma\"])\n",
    "    \n",
    "    critic = Critic(2, hyperparameters[\"hidden_size_critic\"])\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr = hyperparameters[\"lr_critic\"])\n",
    "    critic_scheduler = optim.lr_scheduler.StepLR(critic_optimizer, step_size = hyperparameters[\"step_size\"], gamma = hyperparameters[\"gamma\"])\n",
    "    \n",
    "    \n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    l_actor = []\n",
    "    l_critic = []\n",
    "    l_total = []\n",
    "    all_reward = []\n",
    "    \n",
    "    states_unity = {}\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        states_episode = {}\n",
    "\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        print(\"Training episode: \", episode)\n",
    "\n",
    "        \n",
    "        env.reset()\n",
    "        \n",
    "        states = [v[0] for v in env.states.values()] \n",
    "        state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                    [states[1][0], states[1][1]],\n",
    "                                    [states[2][0], states[2][1]],\n",
    "                                    [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "        \n",
    "        states_episode[str(steps)] = {'0': [1, 1], '1': [1, 1], '2': [1, 1], '3': [1, 1]}\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            steps+=1\n",
    "\n",
    "                \n",
    "            log_probs = []\n",
    "            actions = []\n",
    "            \n",
    "            policy_dist_alpha, policy_dist_beta, sign = actor.forward(state_space)\n",
    "            \n",
    "            value_current = critic.forward(state_space)\n",
    "            for n in range(4):\n",
    "                tmp_alpha = policy_dist_alpha[n].clone()\n",
    "                tmp_beta = policy_dist_beta[n].clone()\n",
    "                \n",
    "                dist = beta(tmp_alpha.detach().numpy(),tmp_beta.detach().numpy())\n",
    "                \n",
    "                action = np.random.beta(tmp_alpha.detach().numpy(), tmp_beta.detach().numpy(), size=2)\n",
    "                log_prob = torch.mean(torch.tensor(dist.logpdf(action)))\n",
    "                log_probs.append(log_prob)\n",
    "                \n",
    "                p = sign[n].clone().detach().numpy()\n",
    "                action_sign = np.random.choice((1,-1), p=p)\n",
    "                action[0] = action[0] * action_sign\n",
    "                \n",
    "                actions.append(action)\n",
    "            \n",
    "            action_space = torch.tensor(actions, dtype=torch.float32)                \n",
    "            \n",
    "            new_state_space, reward, done = env.step(action_space, sign)\n",
    "            \n",
    "            new_state_space = {str(k):v for k, v in new_state_space.items()}\n",
    "            \n",
    "            modified_dict = {}\n",
    "            for k, v in new_state_space.items():\n",
    "                tmp_v = []\n",
    "\n",
    "                \n",
    "                if torch.is_tensor(v[2]):\n",
    "                    tmp_v.append(float(v[2].numpy()))\n",
    "                \n",
    "                if torch.is_tensor(v[3]):\n",
    "                    tmp_v.append(float(v[3].numpy()))\n",
    "                            \n",
    "                modified_dict[k] = tmp_v\n",
    "                \n",
    "            states_episode[str(steps)] = modified_dict\n",
    "            \n",
    "            states = [v[0] for v in env.states.values()] \n",
    "            new_state_space = torch.tensor([[states[0][0], states[0][1]], \n",
    "                                            [states[1][0], states[1][1]],\n",
    "                                            [states[2][0], states[2][1]],\n",
    "                                            [states[3][0], states[3][1]]], dtype=torch.float32)\n",
    "\n",
    "            state_space = new_state_space\n",
    "            \n",
    "            value_next = critic.forward(state_space)\n",
    "            \n",
    "            advantage = (reward + (1-done)*GAMMA*value_next - value_current).squeeze()\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            \n",
    "\n",
    "            tmp = torch.zeros([4, 1], dtype=torch.float32)\n",
    "            log_probs = torch.stack(log_probs, out=tmp)\n",
    "            log_probs = Variable(log_probs, requires_grad=True)\n",
    "\n",
    "#             critic_loss = (advantage**2).mean()\n",
    "            critic_loss = huber_loss(advantage, delta)\n",
    "            \n",
    "            actor_loss = torch.mean(-log_probs*advantage.detach())\n",
    "            \n",
    "            total_loss = critic_loss+actor_loss\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            l_actor.append(actor_loss)\n",
    "            l_critic.append(critic_loss)\n",
    "            l_total.append(total_loss)\n",
    "            \n",
    "            if steps%250 == 0: \n",
    "                print (steps)\n",
    "\n",
    "            if done:\n",
    "                print(\"The total episodic characteristics are as follows:\")\n",
    "                print(\"Critic loss: \",critic_loss)\n",
    "                print(\"Actor loss: \",actor_loss)\n",
    "                print(\"Total loss: \",total_loss)\n",
    "                \n",
    "                states_unity[episode] = states_episode\n",
    "\n",
    "                with open(f'episode_{episode}.txt', 'w') as file:\n",
    "                     file.write(json.dumps(states_episode))\n",
    "                \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "    return episode_rewards, states_unity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episode_rewards,states_unity = train_A2C(env, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.arange(len(episode_rewards)), episode_rewards, s=2)\n",
    "plt.title(\"Total reward per episode (online)\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df=pd.DataFrame({'x': range(1,501), 'y': episode_rewards })\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.plot(df['y'].rolling(5).mean(),label='data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
